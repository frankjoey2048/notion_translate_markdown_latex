# 半监督图卷积网络分类

托马斯 N. Kipf<br>阿姆斯特丹大学<br>T.N.Kipf@uva.nl

马克斯·韦林<br>阿姆斯特丹大学<br>加拿大高级研究院（CIFAR）<br>M.Welling@uva.nl


#### 摘要

我们提出了一种基于高效变体的卷积神经网络、直接在图上操作的可扩展半监督学习方法。我们通过对谱图卷积的局部一阶近似，激发了对我们卷积架构选择的动机。我们的模型在图的边数方面线性扩展，并学习出能够同时编码局部图结构和节点特征的隐藏层表示。在多个关于引用网络和知识图数据集的实验中，我们展示了我们的方法明显优于相关方法。

## 1 引言

我们考虑在图（如引用网络）中对节点（如文档）进行分类的问题，其中标签仅对一小部分节点可用。这个问题可以被框架化为基于图的半监督学习，其中标签信息通过某种显式的基于图的正则化方法在图上平滑（Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012），例如在损失函数中加入图拉普拉斯正则项：

$$
\mathcal{L}=\mathcal{L}_{0}+\lambda \mathcal{L}_{\mathrm{reg}}, \quad \text { 其中 } \quad \mathcal{L}_{\mathrm{reg}}=\sum_{i, j} A_{i j}\left\|f\left(X_{i}\right)-f\left(X_{j}\right)\right\|^{2}=f(X)^{\top} \Delta f(X)
$$

这里，$\mathcal{L}_{0}$表示关于有标签部分的有监督损失，$f(\cdot)$可以是类似神经网络的可微函数，$\lambda$是权重因子，$X$是节点特征向量矩阵，即$X_i$。$\Delta=D-A$表示无归一化的图拉普拉斯矩阵，其中$\mathcal{G}=(\mathcal{V}, \mathcal{E})$为无向图，有$N$个节点$v_i \in \mathcal{V}$，边$\left(v_{i}, v_{j}\right) \in \mathcal{E}$，邻接矩阵$A \in \mathbb{R}^{N \times N}$（可二值化或加权），以及度矩阵$D$，其对角元素$D_{ii}=\sum_{j} A_{ij}$。公式1的推导假设图中相连的节点很可能具有相同的标签。然而，这一假设可能会限制模型能力，因为图中的边未必编码节点相似性，边还可以包含其他信息。

在本工作中，我们直接用神经网络模型$f(X, A)$编码图结构，并用所有带标签的节点的监督目标$\mathcal{L}_0$进行训练，从而避免在损失函数中显式加入图正则化。将$f(\cdot)$条件化在邻接矩阵$A$上，可以让模型在训练中自动将梯度信息传播到带标签和不带标签的节点，并学习它们的表示。

我们的贡献主要有两点。首先，提出一种简单且良好的逐层传播规则，可在直接在图上操作的神经网络模型中实现，并且这规则可以通过谱图卷积的一阶近似来建立理论基础（Hammond et al., 2011）。其次，我们展示了如何利用这种基于图的神经网络模型进行快速且可扩展的节点半监督分类。在多个数据集上的实验表明，我们的模型在分类准确率和运算效率（以实际时间衡量）方面均优于较先进的半监督学习方法。

# 2 图上的快速近似卷积

在本节中，我们将介绍一种特定的基于图的神经网络模型$f(X, A)$的理论动机，接下来会在全文中使用。我们考虑多层图卷积网络（GCN），其每一层的传播规则如下：$$
H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)
$$
这里，$\tilde{A}=A+I_{N}$ 是无向图 $\mathcal{G}$ 添加自连接后的邻接矩阵。$I_{N}$ 是单位矩阵，$\tilde{D}_{i i}=\sum_{j} \tilde{A}_{i j}$，而 $W^{(l)}$ 是特定于第 $l$ 层的可训练权重矩阵。$\sigma(\cdot)$ 表示激活函数，例如 $\operatorname{ReLU}(\cdot)=\max(0, \cdot)$。$H^{(l)} \in \mathbb{R}^{N \times D}$ 是第 $l$ 层的激活矩阵；$H^{(0)}=X$。在下面，我们将展示此传播规则的形式可以通过图的局部谱滤波器的一阶逼近得到启发 ${ }^{1}$（Hammond 等，2011；Defferrard 等，2016）。

### 2.1 谱图卷积

我们考虑定义在图上的谱卷积，具体为在傅里叶域中用由参数 $\theta \in \mathbb{R}^{N}$ 定义的滤波器 $g_{\theta}=\operatorname{diag}(\theta)$ 与信号 $x \in \mathbb{R}^{N}$（每个节点的标量）相乘，即：
$$
g_{\theta} \star x=U g_{\theta} U^{\top} x
$$
其中 $U$ 是归一化图拉普拉斯矩阵 $L=I_{N}-D^{-\frac{1}{2}} A D^{-\frac{1}{2}}=U \Lambda U^{\top}$ 的特征向量矩阵，$\Lambda$ 是其特征值的对角矩阵，$U^{\top} x$ 是 $x$ 的图傅里叶变换。我们可以将 $g_{\theta}$ 理解为一个特征值的函数，即 $g_{\theta}(\Lambda)$。直接计算式（3）在计算上代价较高，因为与特征向量矩阵 $U$ 相乘的复杂度为 $\mathcal{O}(N^2)$。此外，首次计算 $L$ 的特征分解也可能在大规模图上极为昂贵。为避免此问题，在 Hammond 等（2011）中建议，$g_{\theta}(\Lambda)$ 可以用 Chebyshev 多项式 $T_k(x)$ 在 $K$ 阶以内的截断展开来逼近：
$$
g_{\theta^{\prime}}(\Lambda) \approx \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{\Lambda})
$$
其中，$\tilde{\Lambda}=\frac{2}{\lambda_{\max }} \Lambda - I_{N}$ 进行缩放，$\lambda_{\max}$ 表示 $L$ 的最大特征值。$\theta^{\prime} \in \mathbb{R}^{K}$ 现在是切比雪夫系数的向量。切比雪夫多项式递归定义为 $T_k(x) = 2x T_{k-1}(x) - T_{k-2}(x)$，其中 $T_0(x)=1$，$T_1(x)=x$。关于此逼近的详细讨论，读者可参考 Hammond 等（2011）。

回到对信号 $x$ 与滤波器 $g_{\theta^{\prime}}$ 的卷积定义，我们得到：
$$
g_{\theta^{\prime}} \star x \approx \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{L}) x
$$其中 $\tilde{L}=\frac{2}{\lambda_{\max }} L - I_{N}$；这一点可以通过注意到 $\left(U \Lambda U^{\top}\right)^{k}=U \Lambda^{k} U^{\top}$ 这一性质轻松验证。请注意，这个表达式现在是$K$-局部的，因为它是拉普拉斯算子上的$K$次多项式，即它只依赖于距离中心节点最多$K$步的节点（$K$-阶邻域）。计算式（Eq. 5）的复杂度为 $\mathcal{O}(|\mathcal{E}|)$，即与边的数量线性相关。Defferrard 等人（2016）利用这种$K$-局部卷积定义了图上的卷积神经网络。

### 2.2 层级线性模型

因此，可以通过堆叠多个如式（Eq. 5）形式的卷积层构建一个基于图卷积的神经网络模型，每个层后跟一个逐点非线性激活函数。现在，假设我们将层级卷积操作限制在 $K=1$（见式（Eq. 5）），即一个对 $L$ 线性且在图拉普拉斯谱上也是线性函数的函数。

[^0]
[^0]:    ${ }^{1}$ 我们在附录A中提供了基于 Weisfeiler-Lehman 算法（Weisfeiler & Lehmann, 1968）对这种传播规则的另一种解释。

通过这种方式，我们仍然可以通过堆叠多个此类层，恢复丰富类别的卷积滤波器函数，但我们并不局限于由切比雪夫多项式等显式参数化的方式。直观上，我们预期这样的模型可以缓解在具有非常宽的节点度分布的图（如社交网络、引文网络、知识图谱和许多其他真实世界图数据集）中，局部邻域结构过拟合的问题。此外，在固定的计算预算下，这种层级线性形式允许我们构建更深的模型，这一做法在多个领域已被证明可以提高建模能力（He et al., 2016）。

在这种线性形式的GCN中，我们进一步近似 $\lambda_{\max } \approx 2$ ，因为我们可以预期神经网络参数在训练过程中会适应这种尺度变化。在这些近似条件下，式（Eq. 5）简化为：

$$
g_{\theta^{\prime}} \star x \approx \theta_{0}^{\prime} x+\theta_{1}^{\prime}\left(L-I_{N}\right) x=\theta_{0}^{\prime} x-\theta_{1}^{\prime} D^{-\frac{1}{2}} A D^{-\frac{1}{2}} x
$$

具有两个自由参数 $\theta_{0}^{\prime}$ 和 $\theta_{1}^{\prime}$。滤波器参数可以在整个图上共享。连续应用这种形式的滤波器实际上会对节点的$k$阶邻域进行卷积，其中$k$为神经网络模型中的连续滤波操作或卷积层数。

在实际应用中，进一步限制参数的数量以应对过拟合并减少每层的运算量（如矩阵乘法）是有益的。这使我们得到如下表达式：

$$
g_{\theta} \star x \approx \theta\left(I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}\right) x
$$只需一个参数 $\theta=\theta_{0}^{\prime}=-\theta_{1}^{\prime}$。注意到 $I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ 现在具有特征值在区间 $[0,2]$ 中。重复应用此算子可能会在深度神经网络模型中导致数值不稳定以及梯度爆炸或消失。为缓解这一问题，我们引入如下再归一化技巧：$I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$，其中 $\tilde{A}=A+I_{N}$，且 $\tilde{D}_{i i}=\sum_{j} \tilde{A}_{i j}$。

我们可以将该定义推广到具有 $C$ 个输入通道（即每个节点的特征向量为 $C$ 维）和 $F$ 个滤波器或特征映射的信号 $X \in \mathbb{R}^{N \times C}$，如下所示：
$$
Z=\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X \Theta
$$
其中 $\Theta \in \mathbb{R}^{C \times F}$ 现在是滤波参数矩阵，$Z \in \mathbb{R}^{N \times F}$ 是卷积后的信号矩阵。此滤波操作的复杂度为 $\mathcal{O}(|\mathcal{E}| F C)$，因为 $\tilde{A} X$ 可以高效地通过稀疏矩阵与稠密矩阵的乘积实现。

# 3 半监督节点分类

在引入一个简单但灵活的模型 $f(X, A)$ 用于在图上高效传播信息之后，我们可以回到半监督节点分类的问题。如引言所述，我们可以通过对模型 $f(X, A)$ 进行条件化（依赖于数据 $X$ 以及基础图结构的邻接矩阵 $A$）来放宽在基于图的半监督学习中通常所做的某些假设。我们预期在邻接矩阵包含数据 $X$ 中未体现信息的场景下，尤其具有强大效果，例如引文网络中的文献引用连接或知识图谱中的关系。整体模型，即用于半监督学习的多层GNN（图卷积网络），示意图如图1所示。

### 3.1 例子

接下来，我们考虑一个在具有对称邻接矩阵 $A$（二值或加权）图上的两层GNN，用于半监督节点分类。我们首先在预处理步骤中计算 $\tilde{A}=\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$。随后，我们的前向模型可以写成：
$$
Z=f(X, A)=\operatorname{softmax}\left(\tilde{A} \operatorname{ReLU}\left(\tilde{A} X W^{(0)}\right) W^{(1)}\right)
$$
![img-0.jpeg](images/page4_img1.png)

图1：左：多层图卷积网络（GCN）用于半监督学习的示意图，输入层有 $C$ 个通道，输出层有 $F$ 个特征映射。图结构（边用黑线表示）在层间共享，标签用 $Y_{i}$ 表示。右：在 Cora 数据集（Sen 等，2008）上使用 $5\%$ 标签训练的两层GCN隐藏层激活的 t-SNE（Maaten & Hinton, 2008）可视化。颜色代表文档类别。

其中，$W^{(0)} \in \mathbb{R}^{C \times H}$ 为输入到隐藏层的权重矩阵，具有 $H$ 个特征映射；$W^{(1)} \in \mathbb{R}^{H \times F}$ 为隐藏层到输出层的权重矩阵。Softmax 激活函数定义为 $\operatorname{softmax}\left(x_{i}\right)=\frac{1}{\mathcal{Z}} \exp \left(x_{i}\right)$，其中 $\mathcal{Z}=\sum_{i} \exp \left(x_{i}\right)$，按行应用。在半监督多类别分类中，我们将对所有标记的样本计算交叉熵误差：$$
\mathcal{L}=-\sum_{l \in \mathcal{Y}_{L}} \sum_{f=1}^{F} Y_{l f} \ln Z_{l f}
$$

其中 $\mathcal{Y}_{L}$ 是具有标签的节点索引集合。
神经网络的权重 $W^{(0)}$ 和 $W^{(1)}$ 通过梯度下降进行训练。在本工作中，我们采用全数据集的批量梯度下降方法，每次训练迭代都使用完整数据集，这是一种切实可行的方案，只要数据集能存入内存。使用稀疏表示的 $A$ 时，内存需求为 $\mathcal{O}(|\mathcal{E}|)$，即与边数呈线性关系。训练过程中的随机性通过 dropout（Srivastava 等, 2014）引入。关于利用小批量随机梯度下降实现的内存优化扩展，留待未来工作研究。

# 3.2 实现细节

在实际应用中，我们利用 TensorFlow（Abadi 等, 2015）实现 Eq. 9 的高效 GPU 版本——使用稀疏-密集矩阵乘法。这一计算的复杂度为 $\mathcal{O}(|\mathcal{E}| C H F)$，即与图的边数呈线性关系。

## 4 相关工作

我们的模型既受到图结构半监督学习领域的启发，也借鉴了最近关于在图上操作的神经网络的研究。以下是两个领域的相关工作的简要概述。

### 4.1 基于图的半监督学习

近年来，提出了大量利用图表示进行半监督学习的方法，这些方法大致分为两类：一种是利用明确的图拉普拉斯正则化的方法，另一类是基于图嵌入的方案。典型的利用图拉普拉斯正则化的方法包括标签传播（Zhu 等, 2003）、流形正则化（Belkin 等, 2006）以及深度半监督嵌入（Weston 等, 2012）。

[^0]
[^0]： ${ }^{2}$ 复现我们实验的代码已公开在 https://github.com/tkipf/gcn。

近期，研究重点转向借鉴 skip-gram 模型（Mikolov 等, 2013）的方法学习图嵌入。DeepWalk（Perozzi 等, 2014）通过预测节点的局部邻域（采样自随机游走）来学习嵌入。LINE（Tang 等, 2015）和 node2vec（Grover & Leskovec, 2016）则通过更复杂的随机游走或广度优先搜索方案扩展了 DeepWalk。这些方法都需要一个多步骤的流程，包括随机游走生成和半监督训练，每一步都需单独优化。Planetoid（Yang 等, 2016）则通过在嵌入学习过程中引入标签信息，从而减轻了上述流程的复杂性。

# 4.2 图上的神经网络操作于图上的神经网络之前已在 Gori 等人（2005）；Scarselli 等人（2009）中被提出，作为一种递归神经网络的形式。他们的框架需要反复应用压缩映射作为传播函数，直到节点表示达到稳定的固定点。这一限制后来在 Li 等人（2016）中得以缓解，通过引入现代的循环神经网络训练实践到原始的图神经网络框架中。Duvenaud 等人（2015）在图上引入了一种类似卷积的传播规则及图级分类方法。他们的方法要求学习节点度特定的权重矩阵，但这在具有宽节点度分布的大规模图中不适用。我们的模型则采用每层一个统一的权重矩阵，并通过对邻接矩阵的适当归一化（见第3.1节）来处理不同的节点度。

一种与基于图的神经网络进行节点分类相关的方法最近在 Atwood 和 Towsley（2016）中被提出。他们报告的复杂度为 $\mathcal{O}\left(N^{2}\right)$，限制了其潜在应用的范围。在另一种相关模型中，Niepert 等人（2016）将图局部转换成序列，然后输入到传统的一维卷积神经网络中，这需要在预处理步骤中定义节点的排序。

我们的方法基于光谱图卷积神经网络，最初由 Bruna 等人（2014）提出，后由 Defferrard 等人（2016）扩展为具有快速局部卷积的模型。与这些工作不同，我们考虑的是在规模显著更大的网络中进行的传导性节点分类任务。我们展示了在这种设置下，可以对 Bruna 等人（2014）和 Defferrard 等人（2016）原有框架引入一些简化（参见第2.2节），以提高在大规模网络中的可扩展性和分类性能。

## 5 实验

我们在多个实验中测试了我们的模型：引文网络中的半监督文档分类、从知识图中提取的二分图中的半监督实体分类、各种图传播模型的评估以及随机图上的运行时间分析。

### 5.1 数据集

我们紧随 Yang 等人（2016）的实验设置。数据集统计信息总结在表1中。在引文网络数据集——Citeseer、Cora 和 Pubmed（Sen 等，2008）——节点为文档，边为引文连接。标签率表示用于训练的标注节点数除以每个数据集中的总节点数。NELL（Carlson 等，2010；Yang 等，2016）是一个从知识图中提取的二分图数据集，有 55,864 个关系节点和 9,891 个实体节点。

表1：数据集统计信息，详见Yang等人（2016）。

| 数据集   |   类型   | 节点数 |  边数   | 类别数 | 特征数 | 标签率 |
| :------- | :------: | :----: | :-----: | :----: | :----: | :----: |
| Citeseer | 引文网络 | 3,327  |  4,732  |   6    | 3,703  | 0.036  |
| Cora     | 引文网络 | 2,708  |  5,429  |   7    | 1,433  | 0.052  |
| Pubmed   | 引文网络 | 19,717 | 44,338  |   3    |  500   | 0.003  |
| NELL     |  知识图  | 65,755 | 266,144 |  210   | 5,414  | 0.001  |

引文网络 我们考虑三个引文网络数据集：Citeseer、Cora 和 Pubmed（Sen 等，2008）。这些数据集包含每个文档的稀疏词袋特征向量以及文档之间的引文链接列表。我们将引文链接视为（无向）边，并构建一个二值的对称邻接矩阵 $A$。每个文档具有一个类别标签。在训练阶段，我们仅使用每个类别的20个标签，但使用所有的特征向量。NELL NELL 是一个从（Carlson 等人，2010）中引入的知识图谱中提取的数据集。知识图谱是一组由有向带标签的边（关系）连接的实体组成。我们遵循 Yang 等人（2016）描述的预处理方案。对于每个实体对 $\left(e_{1}, r, e_{2}\right)$，我们为其分配单独的关系节点 $r_{1}$ 和 $r_{2}$，表示为 $\left(e_{1}, r_{1}\right)$ 和 $\left(e_{2}, r_{2}\right)$。实体节点由稀疏特征向量描述。我们通过为每个关系节点分配唯一的 one-hot 表示，扩展了 NELL 中的特征数量，实际上每个节点得到一个 61,278 维的稀疏特征向量。在这里的半监督任务中，考虑极端情况：每个类别在训练集中只有一个标记样本。我们从该图构建了一个二值对称邻接矩阵，通过设置条目 $A_{i j}=1$，如果节点 $i$ 和 $j$ 之间存在一条或多条边。

随机图 我们模拟不同规模的随机图数据集，用于测量每个训练轮次的训练时间。对于包含 $N$ 个节点的数据集，我们随机分配 $2N$ 条边以均匀随机的方式。我们采用单位矩阵 $I_{N}$ 作为输入特征矩阵 $X$，暗示采用无特征的方式，模型仅通过每个节点的唯一 one-hot 向量得知节点身份。我们为每个节点添加虚拟标签 $Y_{i}=1$。

# 5.2 实验设置

除非另有说明，我们采用第3.1节描述的两层GCN进行训练，并在含有1,000个标记示例的测试集上评估预测准确率。我们在附录 B 中提供了使用最多10层的更深模型的额外实验。我们选择与Yang等人（2016）相同的数据集划分，并增加一个由500个已标记样本组成的验证集，用于超参数调优（所有层的 dropout 比例、第一层 GCN 的 L2 正则化因子和隐藏单元数）。我们不使用验证集标签进行训练。

对于引文网络数据集，超参数仅在 Cora 上进行优化，并对 Citeseer 和 Pubmed 使用相同的参数集。我们最大训练200轮（训练迭代），使用 Adam（Kingma & Ba, 2015），学习率为0.01，并采用提前停止，窗口大小为10，即如果验证损失在连续10个轮次中没有改善，则停止训练。权重初始化采用 Glorot 和 Bengio（2010）中描述的方法，并相应地对输入特征向量进行（行）标准化。在随机图数据集上，我们使用隐藏层单元数为32，不进行正则化（即不使用 dropout 也不使用 L2 正则）。

### 5.3 基线

我们与Yang等人（2016）中采用的相同基线方法进行比较，即标签传播（LP） (Zhu et al., 2003)、半监督嵌入（SemiEmb）(Weston et al., 2012)、流形正则化（ManiReg）(Belkin et al., 2006) 和基于 skip-gram 的图嵌入（DeepWalk）(Perozzi et al., 2014)。我们省略了 TSVM (Joachims, 1999)，因为它在我们的某个数据集的许多类别中无法扩展。

我们还与 Lu 和 Getoor (2003) 提出的迭代分类算法（ICA）进行比较，该方法结合两个逻辑回归分类器：一个仅使用局部节点特征，另一个结合局部特征和聚合操作进行关系分类，后者如 Sen 等人（2008）所述。我们首先用所有标记训练节点训练局部分类器，然后用它来引导未标记节点的类别标签，用于关系分类器训练。在所有未标记节点（用局部分类器自举得到）上运行带随机节点顺序的迭代分类（关系分类器）共10次。正则化参数（L2）和聚合操作（计数 vs. 比例，见 Sen et al., 2008）根据每个数据集的验证集性能单独确定。

最后，我们与 Planetoid (Yang 等人，2016) 进行比较，始终选择其表现最优的模型变体（传导式或归纳式）作为基线。

# 6 结果

### 6.1 半监督节点分类结果总结见表2。报告的数字为分类准确率（百分比）。对于ICA，我们报告100次随机节点顺序的平均准确率。所有其他基线方法的结果均取自Planetoid论文（Yang et al., 2016）。Planetoid* 表示其论文中在对应数据集上表现最好的模型。

表2：以分类准确率（百分比）为指标的结果总结。

| 方法            | Citeseer                          | Cora                              | Pubmed                             | NELL                               |
| :-------------- | :-------------------------------- | :-------------------------------- | :--------------------------------- | :--------------------------------- |
| ManiReg [3]     | 60.1                              | 59.5                              | 70.7                               | 21.8                               |
| SemiEmb [28]    | 59.6                              | 59.0                              | 71.1                               | 26.7                               |
| LP [32]         | 45.3                              | 68.0                              | 63.0                               | 26.5                               |
| DeepWalk [22]   | 43.2                              | 67.2                              | 65.3                               | 58.1                               |
| ICA [18]        | 69.1                              | 75.1                              | 73.9                               | 23.1                               |
| Planetoid* [29] | $64.7(26 \mathrm{~s})$            | $75.7(13 \mathrm{~s})$            | $77.2(25 \mathrm{~s})$             | $61.9(185 \mathrm{~s})$            |
| GCN（本文）     | $\mathbf{7 0 . 3}(7 \mathrm{~s})$ | $\mathbf{8 1 . 5}(4 \mathrm{~s})$ | $\mathbf{7 9 . 0}(38 \mathrm{~s})$ | $\mathbf{6 6 . 0}(48 \mathrm{~s})$ |
| GCN（随机划分） | $67.9 \pm 0.5$                    | $80.1 \pm 0.5$                    | $78.9 \pm 0.7$                     | $58.4 \pm 1.7$                     |

我们进一步报告了我们的方法（包括验证误差评估）以及Planetoid的墙钟训练时间（秒，括号内）直到收敛。对于后者，我们使用了作者提供的实现 ${ }^{3}$，并在与我们的GCN模型相同的硬件（带GPU）上进行训练。我们在Yang等人（2016）采用的相同数据集划分上训练和测试了模型，并报告100次随机权重初始化的平均准确率。对于Citeseer、Cora和Pubmed，我们采用的超参数为：0.5（dropout比例）、$5 \times 10^{-4}$（L2正则化）和16（隐藏单元数）；对于NELL，则为：0.1（dropout比例）、$1 \times 10^{-5}$（L2正则化）和64（隐藏单元数）。

此外，我们还在10个随机抽取且大小与Yang等人（2016）相同的数据集划分上，报告模型的性能，标记为GCN（随机划分）。这里，我们报告测试集划分上预测准确率的平均值和标准误差（百分比）。

### 6.2 传播模型的评估

我们比较了提出的每层传播模型的不同变体在引文网络数据集上的表现。我们遵循前节所述的实验设置。结果总结在表3中。我们原始GCN模型的传播模型采用重归一化技巧（加粗）。在所有其他情况下，两个神经网络层的传播模型都被替换为对应的模型。报告的数字为100次重复运行（随机初始化权重矩阵）后的平均分类准确率。在有多个变量 $\Theta$ 的情况下，每层对所有第一层的权重矩阵施加L2正则化。

表3：传播模型的比较。| 描述 | 传播模型 | Citeseer | Cora | Pubmed |
| :-- | :--: | :--: | :--: | :--: |
| 切比雪夫滤波器（式5） | $K=3$ | $ \sum_{k=0}^{K} T_{k}(\hat{L}) X \Theta_{k} $ | 69.8 | 79.5 |
|  | $K=2$ | 69.6 | 81.2 | 73.8 |
| 一阶模型（式6） | $X \Theta_{0}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}} X \Theta_{1}$ | 68.3 | 80.0 | 77.5 |
| 单参数（式7） | $\left(I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}\right) X \Theta$ | 69.3 | 79.2 | 77.4 |
| 归一化技巧（式8） | $\hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}} X \Theta$ | $\mathbf{7 0 . 3}$ | $\mathbf{8 1 . 5}$ | $\mathbf{7 9 . 0}$ |
| 只用一阶项 | $D^{-\frac{1}{2}} A D^{-\frac{1}{2}} X \Theta$ | 68.7 | 80.5 | 77.8 |
| 多层感知机 | $X \Theta$ | 46.5 | 55.1 | 71.4 |

[^0]
[^0]: ${ }^{3}$ https://github.com/kimiyoung/planetoid

### 6.3 每轮训练时间

这里，我们报告在模拟随机图上进行100轮训练（正向传播、交叉熵计算、反向传播）的平均时间（秒，墙钟时间）。关于这些实验所用随机图数据集的详细描述，请参见第5.1节。我们比较在GPU和仅使用CPU的TensorFlow实现（Abadi等，2015）上的结果。图2总结了这些结果。
![img-1.jpeg](images/page8_img1.png)

图2：随机图每轮训练的墙钟时间。(*)表示内存溢出错误。

# 7 讨论

### 7.1 半监督模型

在这里展示的实验结果表明，我们的半监督节点分类方法明显优于近期相关方法。基于图拉普拉斯正则化的方法（Zhu等，2003；Belkin等，2006；Weston等，2012）很可能受到限制，因为它们假设边仅表示节点的相似性。另一方面，基于跳字模型的方法受限于其多步流水线结构，难以优化。我们提出的模型能克服这两方面的限制，同时在效率（以墙钟时间衡量）方面仍优于相关方法。每一层中邻居节点信息的传播增强了分类性能，相较于只聚合标签信息的ICA（Lu & Getoor，2003）等方法。

我们还进一步证明，提出的归一化传播模型（式8）在多项数据集上比单纯的$1^{\text{st}}$阶模型（式6）或使用切比雪夫多项式的高阶图卷积模型（式5）具有更高的效率（参数和运算，如乘法或加法更少）和更优的预测性能。

### 7.2 限制与未来工作

在这里，我们列出当前模型的几个局限，并展望未来可能的改进方向。

内存需求：在采用全批次梯度下降的现有设置中，内存需求随数据集规模线性增长。我们已经表明，对于不能放入GPU内存的大型图，使用CPU进行训练仍然是可行的。小批量随机梯度下降可以缓解这一问题。然而，生成小批量的过程应考虑到GCN模型的层数，因为具有$K$层的GCN的第$K$阶邻域必须存储在内存中以确保准确执行。对于非常庞大且密集连接的图数据集，可能还需要采用其他近似方法。有向边和边特征 我们的框架目前不原生支持边特征，并且仅限于无向图（有权或无权）。然而，在 NELL 上的结果显示，通过将原始有向图表示为具有额外节点的无向二分图（这些节点代表原图中的边），可以处理有向边和边特征（详见第 5.1 节）。

假设限制 通过第 2 节引入的近似，我们隐式假设局部性（依赖于具有 K 层的 GCN 的第 K 阶邻域）以及自连接与邻居节点边的等重要性。然而，对于某些数据集，引入一个调节参数 λ 可能会更有益，其在定义 \(\tilde{A}\) 时起到类似于半监督设置中监督与无监督损失的权衡作用（见公式 1）。然而，在这里，它可以通过梯度下降进行学习。

# 8 结论

我们提出了一种用于图结构数据半监督分类的创新方法。我们的 GCN 模型采用基于图的谱卷积一次阶近似的高效逐层传播规则。在多个网络数据集上的实验表明，所提的 GCN 模型能够有效编码图结构和节点特征，具有良好的半监督分类性能。在此设置下，我们的模型比几种近期提出的方法都取得了显著的改善，同时计算效率也很高。

## 致谢

我们感谢 Christos Louizos、Taco Cohen、Joan Bruna、Zhilin Yang、Dave Herman、Pramod Sinha 和 Abdul-Saboor Sheikh 的有益讨论。本研究由 SAP 资助。

## 参考文献

Martín Abadi 等人. TensorFlow：在异构系统上的大规模机器学习, 2015.

James Atwood 和 Don Towsley. 扩散卷积神经网络. 在神经信息处理系统进展（NIPS），2016.

Mikhail Belkin、Partha Niyogi 和 Vikas Sindhwani. 流形正则化：一种从标记和未标记样本学习的几何框架. 机器学习研究杂志（JMLR），2006 年 11 月第 7 期：2399-2434.

Ulrik Brandes、Daniel Delling、Marco Gaertler、Robert Gorke、Martin Hoefer、Zoran Nikoloski 和 Dorothea Wagner. 关于模块性聚类. IEEE 知识与数据工程学报，2008 年第 20 巻第 2 期：172-188.

Joan Bruna、Wojciech Zaremba、Arthur Szlam 和 Yann LeCun. 光谱网络和图上的局部连接网络. 2014 年国际学习表示会议（ICLR）。

Andrew Carlson、Justin Betteridge、Bryan Kisiel、Burr Settles、Estevam R. Hruschka Jr 和 Tom M. Mitchell. 迈向无终结语言学习的体系结构. AAAI，第 5 卷，第 3 页，2010.

Michaël Defferrard、Xavier Bresson 和 Pierre Vandergheynst. 具有快速局部谱过滤的图卷积神经网络. 在神经信息处理系统进展（NIPS），2016.

Brendan L. Douglas. Weisfeiler-Lehman 方法与图同构检测. arXiv 预印本 arXiv:1101.5211, 2011.David K. Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, 和 Ryan P. Adams. 图上的卷积网络用于学习分子指纹. 载于 《神经信息处理系统进展》（NIPS），第2224-2232页，2015年。

Xavier Glorot 和 Yoshua Bengio. 理解深层前馈神经网络训练难点. 载于 AISTATS, 第9卷, 第249-256页, 2010年。

Marco Gori, Gabriele Monfardini, 和 Franco Scarselli. 图域中的新型学习模型. 载于 2005年IEEE国际神经网络联合会议论文集, 第2卷, 第729-734页. IEEE, 2005年。

Aditya Grover 和 Jure Leskovec. node2vec：网络的可扩展特征学习. 载于第22届ACM知识发现与数据挖掘国际会议论文集. ACM, 2016。

David K. Hammond, Pierre Vandergheynst, 和 Rémi Gribonval. 通过谱图理论在图上构建的小波. 《应用与计算调和分析》, 第30卷(2): 第129-150页, 2011年。

Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 用于图像识别的深度残差学习. 载于IEEE计算机视觉与模式识别会议（CVPR）, 2016年。

Thorsten Joachims. 利用支持向量机进行文本分类的转导推断. 载于国际机器学习会议（ICML）, 第99卷, 第200-209页, 1999年。

Diederik P. Kingma 和 Jimmy Lei Ba. Adam：一种随机优化方法. 载于国际学习表征会议（ICLR）, 2015年。

Yujia Li, Daniel Tarlow, Marc Brockschmidt, 和 Richard Zemel. 门控图序列神经网络. 载于国际学习表征会议（ICLR）, 2016年。

Qing Lu 和 Lise Getoor. 基于链接的分类. 载于国际机器学习会议（ICML）, 第3卷, 第496-503页, 2003年。

Laurens van der Maaten 和 Geoffrey Hinton. 使用 t-SNE 进行数据可视化. 《机器学习研究杂志》（JMLR）, 第9卷(11月): 第2579-2605页, 2008年。

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, 和 Jeff Dean. 词语及短语的分布式表征及其组合性. 载于《神经信息处理系统进展》（NIPS）, 第3111-3119页, 2013年。

Mathias Niepert, Mohamed Ahmed, 和 Konstantin Kutzkov. 图卷积神经网络的学习. 载于国际机器学习会议（ICML）, 2016年。

Bryan Perozzi, Rami Al-Rfou, 和 Steven Skiena. Deepwalk：社会关系的在线学习. 载于第20届ACM知识发现与数据挖掘国际会议论文集, 第701-710页. ACM, 2014。

Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, 和 Gabriele Monfardini. 图神经网络模型. 《IEEE神经网络杂志》, 第20卷(1): 第61-80页, 2009年。

Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, 和 Tina Eliassi-Rad. 网络数据中的集体分类. 《人工智能杂志》（AI magazine）, 第29卷(3): 93, 2008年。

Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, 和 Ruslan Salakhutdinov. Dropout：一种防止神经网络过拟合的简便方法. 《机器学习研究杂志》（JMLR）, 第15卷(1): 第1929-1958页, 2014年。

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, 和 Qiaozhu Mei. Line：大规模信息网络嵌入. 载于第24届万维网国际会议论文集, 第1067-1077页. ACM, 2015年。Boris Weisfeiler 和 A. A. Lehmann. 图的归一化到标准形式的简化以及在此简化过程中出现的代数结构。科学技术信息，2(9):12-16，1968。

Jason Weston, Frédéric Ratle, Hossein Mobahi, 和 Ronan Collobert. 通过半监督嵌入进行深度学习。《神经网络：行业技巧》，第 639-655 页。 Springer，2012。

Zhilin Yang, William Cohen, 和 Ruslan Salakhutdinov. 重新审视带图嵌入的半监督学习。 2016 年机器学习国际会议（ICML）。

Wayne W. Zachary. 小团体中冲突与分裂的信息流模型。人类学研究杂志，第 452-473 页，1977。

Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, 和 Bernhard Schölkopf. 具有局部与整体一致性的学习。在神经信息处理系统进展（NIPS），第 16 卷， 第 321-328 页，2004。

Xiaojin Zhu, Zoubin Ghahramani, 和 John Lafferty. 使用高斯场和调和函数的半监督学习。 机器学习国际会议（ICML），第 3 卷， 第 912-919 页，2003。

# 与 WeisFeiler-Lehman 算法的关系

针对图结构数据的神经网络模型理应能够学习图中节点的表示，同时考虑到图结构和节点特征描述。由 Weisfeiler 和 Lehmann（1968）提出的单维 Weisfeiler-Lehman（WL-1）算法提供了一个在已知图和（可选）离散起始节点标签的情况下唯一确定节点标签分配的框架：

```
算法 1：WL-1 算法（Weisfeiler & Lehmann，1968）
输入：初始节点着色 \(\left(h_{1}^{(0)}, h_{2}^{(0)}, \ldots, h_{N}^{(0)}\right)\)
输出：最终节点着色 \(\left(h_{1}^{(T)}, h_{2}^{(T)}, \ldots, h_{N}^{(T)}\right)\)
\(\mathrm{t} \leftarrow 0\);
重复
    对于 \(v_{i} \in \mathcal{V}\) 执行
        \(h_{i}^{(t+1)} \leftarrow \operatorname{hash}\left(\sum_{j \in \mathcal{N}_{i}} h_{j}^{(t)}\right)\);
    \(t \leftarrow t+1\);
直到达到稳定的节点着色；
```

这里，$h_{i}^{(t)}$ 表示节点 $v_{i}$（在第 $t$ 次迭代中的）染色（标签分配），$\mathcal{N}_{i}$ 是其邻居节点的集合（不论图是否为每个节点包含自连接）。hash$(\cdot)$ 是一个哈希函数。关于 WL-1 算法的深入数学讨论参见，例如 Douglas（2011）。

我们可以将算法 1 中的哈希函数替换为具有可训练参数的神经网络层类可微函数，如下：

$$
h_{i}^{(l+1)}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \frac{1}{c_{i j}} h_{j}^{(l)} W^{(l)}\right)
$$其中 $c_{i j}$ 是为边 $\left(v_{i}, v_{j}\right)$ 适当选取的归一化常数。此外，我们现在可以将 $h_{i}^{(l)}$ 视为第 $l$ 层神经网络中节点 $i$ 的激活向量。$W^{(l)}$ 是层特定的权重矩阵，$\sigma(\cdot)$ 表示一个可微的非线性激活函数。

通过选择 $c_{i j}=\sqrt{d_{i} d_{j}}$，其中 $d_{i}=\left|\mathcal{N}_{i}\right|$ 表示节点 $v_{i}$ 的度数，我们在向量形式下恢复了我们的图卷积网络（GCN）模型的传播规则（见式 2）${ }^{5}$。

这大致可以让我们将我们的 GCN 模型理解为对图上的一维 Weisfeiler-Lehman 算法的可微且参数化的推广。

## A. 使用随机权重的单节点嵌入

根据 Weisfeiler-Lehman 算法的类比，我们可以理解，即使是带有随机权重、未经过训练的 GCN 模型，也可以作为对图中节点的强大特征提取器。例如，考虑以下三层 GCN 模型：
$$
Z=\tanh \left(\hat{A} \tanh \left(\hat{A} \tanh \left(\hat{A} X W^{(0)}\right) W^{(1)}\right) W^{(2)}\right)
$$
其中权重矩阵 $W^{(l)}$，使用 Glorot 与 Bengio（2010）中描述的初始化方法随机初始化。$\hat{A}$、$X$ 和 $Z$ 的定义与第3.1节中相同。

我们将在 Zachary 的空手道俱乐部网络（Zachary, 1977）上应用此模型。该图包含34个节点，通过154条（无向、无权重）边连接。每个节点都被标记为四个类别之一，这些类别通过基于模块度的聚类方法获得（Brandes 等，2008）。请参见图3a以获取示意。

[^0]
[^0]:    ${ }^{5}$ 注意，我们在此隐含假设每个节点的自连接已经添加（以保持符号简洁）。

![img-2.jpeg](images/page12_img1.png)

图3：左：Zachary 的空手道俱乐部网络（Zachary，1977），颜色表示通过基于模块度的聚类（Brandes 等，2008）得到的社区。右：在空手道俱乐部网络上应用未训练的三层 GCN 模型（式13）所获得的嵌入。建议用电脑屏幕观看以获得最佳效果。

我们采用无特征的方法，将 $X=I_{N}$，其中 $I_{N}$ 是 $N\times N$ 的单位矩阵。$N$ 为图中的节点数。注意，节点被随机排序（即排序不包含任何信息）。此外，我们选择隐藏层维度 ${ }^{6}$ 为4，输出为二维（以便可以直接在二维图中进行可视化）。
图3b展示了从未训练的 GCN 模型在空手道俱乐部网络上获得的节点嵌入（输出 $Z$）的典型例子。这些结果与通过更昂贵的无监督训练程序得到的 DeepWalk（Perozzi 等，2014）嵌入相当。

# A. 2 半监督节点嵌入

在这个应用于空手道俱乐部网络的简单 GCN 示例中，观察嵌入在训练过程中对半监督分类任务的反应具有趣味性。这种可视化（见图4）提供了有关 GCN 模型如何利用图结构（以及后续层从图结构中提取的特征）学习对分类任务有用的嵌入的见解。

我们考虑如下半监督学习设置：在模型（式13）上添加一个 softmax 层，只用每个类别的一个标记样本进行训练（即总共4个标记节点）。我们使用 Adam（Kingma & Ba, 2015），以 0.01 的学习率，在交叉熵损失上训练300次迭代。图 4 展示了节点嵌入在多次训练迭代中的演变。该模型能够仅依靠最小的监督和图结构，线性区分不同的社区。完整训练过程的视频可以在我们的网站上找到 ${ }^{7}$。

[^0]
[^0]: ${ }^{6}$ 我们最初尝试了隐藏层维度为 2（与输出层相同），但观察到维度为 4 时，$\tanh (\cdot)$ 单元的饱和频率较低，因此视觉效果更佳。
${ }^{7}$ http://tkipf.github.io/graph-convolutional-networks/

![img-3.jpeg](images/page13_img1.png)

图 4：通过多次半监督训练迭代后，从 GCN 模型获得的空手道俱乐部网络节点嵌入的演变。颜色表示类别。训练期间提供标签的节点（每个类别一个）被高亮显示（灰色轮廓）。节点之间的灰色连线表示图边。建议在电脑屏幕上查看。

# B 在模型深度上的实验

在这些实验中，我们研究了模型深度（层数）对分类性能的影响。我们在 Cora、Citeseer 和 Pubmed 数据集（Sen 等，2008）上使用所有标签进行了 5 折交叉验证实验，并报告结果。除了标准的 GCN 模型（式 2），我们还报告了采用残差连接（He 等，2016）变体的结果，该连接在隐藏层之间有助于训练更深的模型，通过让模型能够携带前一层输入中的信息：
$$
H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)+H^{(l)}
$$

在每个交叉验证划分中，我们训练 400 个 epoch（不提前停止），使用 Adam 优化器（Kingma & Ba, 2015），学习率为 0.01。其他超参数的选择如下：0.5（Dropout 率，第一层和最后一层）、$5 \times 10^{-4}$（L2 正则化，第一层）、16（每个隐藏层的单元数）和 0.01（学习率）。结果汇总见图 5。

![img-4.jpeg](images/page14_img1.png)

图 5：模型深度（层数）对分类性能的影响。标记点表示 5 折交叉验证的平均分类准确率（训练与测试）。阴影区域表示标准误差。我们展示了标准 GCN 模型（虚线）和在隐藏层之间增加残差连接的模型（实线）（He 等，2016）的结果。

对于所考虑的数据集，最佳结果是在 2 层或 3 层模型上获得的。我们观察到，深于 7 层的模型在没有使用残差连接的情况下训练可能变得困难，因为每增加一层，节点的有效上下文大小就会增加到其 $K^{\text{th}}$ 阶邻居的规模（对于具有 $K$ 层的模型）。此外，随着模型深度的增加，参数数量也会增多，可能导致过拟合问题。